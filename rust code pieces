[dependencies]
polars = "0.15"
#Using polars for DataFrame Operations, have polars in Cargo.toml
[dependencies]
reqwest = { version = "0.11", features = ["json", "blocking"] }
serde = "1.0"
serde_json = "1.0"
chrono = "0.4"
plotters = "0.3"
use chrono::NaiveDate;
use plotters::prelude::*;
use reqwest::blocking::Client;
use serde::{Deserialize, Serialize};

#[derive(Debug, Deserialize)]
struct StockData {
    timestamp: String,
    open: f64,
    high: f64,
    low: f64,
    close: f64,
    volume: i64,
}

#[derive(Debug, Serialize)]
struct ApiQuery {
    function: String,
    symbol: String,
    apikey: String,
}

fn fetch_stock_data(symbol: &str) -> Vec<StockData> {
    let client = Client::new();
    let query = ApiQuery {
        function: "TIME_SERIES_DAILY".to_string(),
        symbol: symbol.to_string(),
        apikey: "5VXNB3Gg6YPwP6XWdyCDZEfB3aIRmudD".to_string(),
    };

    let res = client.get("https://www.alphavantage.co/query")
        .query(&query)
        .send()
        .unwrap()
        .json::<serde_json::Value>()
        .unwrap();

    let mut data = Vec::new();

    if let Some(series) = res["Time Series (Daily)"].as_object() {
        for (date, values) in series {
            let stock_data = StockData {
                timestamp: date.to_string(),
                open: values["1. open"].as_str().unwrap().parse::<f64>().unwrap(),
                high: values["2. high"].as_str().unwrap().parse::<f64>().unwrap(),
                low: values["3. low"].as_str().unwrap().parse::<f64>().unwrap(),
                close: values["4. close"].as_str().unwrap().parse::<f64>().unwrap(),
                volume: values["5. volume"].as_str().unwrap().parse::<i64>().unwrap(),
            };
            data.push(stock_data);
        }
    }

    data
}

fn plot_data(data: &[StockData]) -> Result<(), Box<dyn std::error::Error>> {
    let root = BitMapBackend::new("stock_data.png", (1280, 720)).into_drawing_area();
    root.fill(&WHITE)?;

    let max_high = data.iter().max_by(|x, y| x.high.partial_cmp(&y.high).unwrap()).unwrap().high;
    let min_low = data.iter().min_by(|x, y| x.low.partial_cmp(&y.low).unwrap()).unwrap().low;

    let mut chart = ChartBuilder::on(&root)
        .caption("Stock Price", ("sans-serif", 50).into_font())
        .x_label_area_size(35)
        .y_label_area_size(40)
        .margin(5)
        .build_ranged(
            NaiveDate::parse_from_str(&data.first().unwrap().timestamp, "%Y-%m-%d")?..NaiveDate::parse_from_str(&data.last().unwrap().timestamp, "%Y-%m-%d")?,
            min_low..max_high,
        )?;

    chart.configure_mesh().draw()?;

    chart.draw_series(LineSeries::new(
        data.iter().map(|x| {
            (
                NaiveDate::parse_from_str(&x.timestamp, "%Y-%m-%d").unwrap(),
                x.close,
            )
        }),
        &RED,
    ))?;

    root.present()?;

    Ok(())
}

fn main() {
    let data = fetch_stock_data("AAPL");
    plot_data(&data).unwrap();
}
  [dependencies]
diesel = { version = "1.4.8", features = ["sqlite"] }
dotenv = "0.15"
serde = "1.0"
serde_json = "1.0"
reqwest = { version = "0.11", features = ["json", "blocking"] }
chrono = { version = "0.4", features = ["serde"] }
plotters = "0.3"
cargo install diesel_cli --no-default-features --features sqlite
cargo install diesel_cli
echo DATABASE_URL=mysql://root:123456@localhost/diesel_learn > .env
embed_migrations!("../migrations");
diesel setup
diesel migration generate create_stocks
CREATE TABLE stocks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    date DATE NOT NULL,
    open REAL NOT NULL,
    high REAL NOT NULL,
    low REAL NOT NULL,
    close REAL NOT NULL,
    volume BIGINT NOT NULL
);
DROP TABLE stocks;
diesel migration run
[print_schema]
file = "src/schema.rs"
with_docs = true
use chrono::NaiveDate;
use diesel::prelude::*;
use diesel::sqlite::SqliteConnection;
use dotenv::dotenv;
use plotters::prelude::*;
use reqwest::blocking::Client;
use serde::{Deserialize, Serialize};
use std::env;

#[derive(Debug, Deserialize)]
struct StockData {
    timestamp: String,
    open: f64,
    high: f64,
    low: f64,
    close: f64,
    volume: i64,
}

#[derive(Debug, Serialize, Insertable)]
#[table_name = "stocks"]
struct NewStock {
    date: NaiveDate,
    open: f64,
    high: f64,
    low: f64,
    close: f64,
    volume: i64,
}

#[derive(Debug, Serialize)]
struct ApiQuery {
    function: String,
    symbol: String,
    apikey: String,
}

fn establish_connection() -> SqliteConnection {
    dotenv().ok(); // Read .env file if present
    let database_url = env::var("DATABASE_URL").expect("DATABASE_URL must be set");
    SqliteConnection::establish(&database_url).expect("Error connecting to database")
}

fn save_stock_data(connection: &SqliteConnection, data: &[NewStock]) {
    use schema::stocks::dsl::*;
    diesel::insert_into(stocks)
        .values(data)
        .execute(connection)
        .expect("Error saving stock data");
}

fn fetch_stock_data(symbol: &str) -> Vec<StockData> {
    // Previous fetching logic remains the same
}

fn main() {
    let connection = establish_connection();
    let data = fetch_stock_data("AAPL");

    let new_stocks: Vec<NewStock> = data.iter().map(|d| NewStock {
        date: NaiveDate::parse_from_str(&d.timestamp, "%Y-%m-%d").unwrap(),
        open: d.open,
        high: d.high,
        low: d.low,
        close: d.close,
        volume: d.volume,
    }).collect();

    save_stock_data(&connection, &new_stocks);
    plot_data(&data).unwrap();
}

fn plot_data(data: &[StockData]) -> Result<(), Box<dyn std::error::Error>> {
    // Previous plotting logic remains the same
}
use polars::prelude::*;
use std::error::Error;

fn main() -> Result<(), Box<dyn Error>> {
    let df = CsvReader::from_path(".csv")?
        .infer_schema(None)
        .finish()?;

    println!("{:?}", df);
    Ok(())
}
[dependencies]
polars = { version = "0.24", features = ["lazy", "csv-file"] }
ta = "0.4.0"
plotters = "0.3.1"
chrono = "0.4"
use polars::prelude::*;
use ta::{indicators::*, Next};
use plotters::prelude::*;
use chrono::NaiveDate;
use std::error::Error;

fn main() -> Result<(), Box<dyn Error>> {
    // Load data
    let mut df = CsvReader::from_path("000001.csv")?
        .infer_schema(None)
        .has_header(true)
        .finish()?;

    // Parse dates and select data
    df.may_apply("date", |s| s.utf8()?.strptime("%Y-%m-%d", |_| DataType::Date))?;
    let df = df.lazy().with_column(col("date").cast(DataType::Datetime));

    // Calculating MACD
    let close = df.column("close")?.f64()?;
    let macd = MACD::new(12, 26, 9).next_all(&close);
    let df = df.with_column(Series::new("macd", macd));

    // Calculating moving averages
    let ma10 = close.iter().map(|&x| x).collect::<Vec<_>>().windows(10).map(|w| w.iter().sum::<f64>() / 10.0).collect::<Vec<_>>();
    let ma30 = close.iter().map(|&x| x).collect::<Vec<_>>().windows(30).map(|w| w.iter().sum::<f64>() / 30.0).collect::<Vec<_>>();
    df.with_columns(vec![Series::new("ma10", ma10), Series::new("ma30", ma30)]);

    // Setup plot
    let root = BitMapBackend::new("chart.png", (1280, 960)).into_drawing_area();
    root.fill(&WHITE)?;

    let mut chart = ChartBuilder::on(&root)
        .caption("Stock Analysis", ("sans-serif", 50).into_font())
        .set_label_area_size(LabelAreaPosition::Left, 40)
        .set_label_area_size(LabelAreaPosition::Bottom, 40)
        .build_cartesian_2d(0..1000, 30.0..90.0)?;

    chart.configure_mesh().draw()?;

    // Example: Plotting MA10
    chart.draw_series(LineSeries::new(
        (0..).zip(ma10.iter().cloned()),
        &RED,
    ))?;

    Ok(())
}
[dependencies]
tch = "0.4"
extern crate tch;
use tch::{nn, nn::Module, nn::OptimizerConfig, Device, Tensor};

fn lstm_model(vs: &nn::Path, input_size: i64, hidden_size: i64, num_layers: i64) -> nn::LSTM {
    nn::lstm(vs, input_size, hidden_size, num_layers)
}

fn main() {
    let vs = nn::VarStore::new(Device::cuda_if_available());
    let lstm_layer = lstm_model(&vs.root(), /*input_size=*/ 10, /*hidden_size=*/ 250, /*num_layers=*/ 2);

    # Example input tensor
    let input = Tensor::randn(&[10, 1, 10], (tch::Kind::Float, Device::Cuda(0)));
    let (output, _) = lstm_layer.seq_init(input);

    println!("{:?}", output);
}
extern crate csv;
extern crate serde;

use ndarray::Array2;
use std::error::Error;

#Data Preprocessing

fn read_csv_to_ndarray(file_path: &str) -> Result<Array2<f32>, Box<dyn Error>> {
    let mut rdr = csv::Reader::from_path(file_path)?;
    let mut data = Vec::new();

    for result in rdr.deserialize() {
        let record: Vec<f32> = result?;
        data.push(record);
    }

    let rows = data.len();
    let cols = if rows > 0 { data[0].len() } else { 0 };
    let flattened_data: Vec<f32> = data.into_iter().flatten().collect();
    let ndarray = Array2::from_shape_vec((rows, cols), flattened_data)?;

    Ok(ndarray)
}
extern crate tch;
use tch::{nn, nn::Module, nn::OptimizerConfig, Device, Tensor};

#training loop
fn train_model() -> Result<(), Box<dyn std::error::Error>> {
    let vs = nn::VarStore::new(Device::cuda_if_available());
    let lstm = lstm_model(&vs.root(), 10, 250, 2); // Example model
    let mut opt = nn::Adam::default().build(&vs, 1e-3)?;

    for epoch in 1..=10 {
        let input = Tensor::randn(&[10, 1, 10], (tch::Kind::Float, Device::Cuda(0))); // Example input
        let target = Tensor::randn(&[10, 1], (tch::Kind::Float, Device::Cuda(0))); // Example target

        let (output, _) = lstm.seq_init(input);
        let loss = output.mse_loss(&target, tch::Reduction::Mean);

        opt.zero_grad();
        loss.backward();
        opt.step();

        println!("Epoch: {} Loss: {:?}", epoch, f64::from(&loss));
    }

    vs.save("model.ot")?;
    Ok(())
}
use std::path::Prefix::Verbatim;


pub use mongodb::{
    bson::{doc, Bson, Document},
    error::Result,
    options::{
        DeleteOptions, FindOneOptions, FindOptions, InsertOneOptions, UpdateModifications,
        UpdateOptions,ClientOptions
    },
    results::{DeleteResult, InsertManyResult, InsertOneResult, UpdateResult},
    sync::{Client, Collection, Cursor, Database},
};
use chrono::{DateTime, FixedOffset, NaiveDate, NaiveDateTime, NaiveTime, TimeZone, Utc};


use mifi_rs::kline::{future_day, future_min, stock_day, stock_min};

#financial analysis: Chrono Crate, mifi_rs Crate

fn to_timestamp(date: String) -> i64{
    Utc.datetime_from_str(format!("{} 00:00:00", date).as_str(), "%Y-%m-%d %H:%M:%S")
    .unwrap()
    .timestamp() -28800
}
#time
pub struct QAMongoClient {
    pub uri: String,
    pub database: Database,
}
#client
